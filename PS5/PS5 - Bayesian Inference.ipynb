{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "# PS 5 - Bayesian Inference: The Coin of Infinite Hypotheses\n",
    "\n",
    "In this problem set, we will explore **Bayesian inference** — a principled framework for updating beliefs based on evidence. We'll use a simple example: estimating the bias of a coin from observed flips.\n",
    "\n",
    "## The Story\n",
    "\n",
    "One day, you find yourself standing in a musty room in the back of an old magic shop. Heavy velvet curtains cover the dirty windows, reluctantly letting in a few narrow beams of light, which succeed only in illuminating layers of dust suspended in the stale air. You glance around cautiously, trying not to invite scrutiny from the watchful shopkeep. Your eyes dance over row upon row of leather-bound books, resting on the rich mahogany shelves lining each wall.\n",
    "\n",
    "Before you can even turn around, the shopkeep anticipates your question. \"You've come for a coin, haven't you?\" His voice sounds strangely distant. You nod, swallowing. He totters past you, one of his legs struggling to keep up with the other. Reaching a black armoire in the corner of the room — how did you not notice it before? — he stops. The shopkeep slowly opens one of the drawers, revealing a beautiful, glimmering coin.\n",
    "\n",
    "Your father's deep, soulful voice echoes in your head: *\"Fetch me a coin of Azeroth, child, but only if its probability of landing heads is between 0.55 and 0.75.\"*\n",
    "\n",
    "---\n",
    "\n",
    "**Is this coin what father asked for?** How can we determine if this coin matches his request?\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Let $\\theta$ denote the coin's probability of landing heads on each toss. Our task is to **infer** $\\theta$ from observed data. This is a learning problem where:\n",
    "\n",
    "- **Hypothesis space**: All possible values of $\\theta$ from 0 to 1 (infinitely many hypotheses!)\n",
    "- **Data**: A sequence of coin flips (heads = 1, tails = 0)\n",
    "- **Goal**: Estimate $\\theta$ as accurately as possible\n",
    "\n",
    "We'll compare three estimation methods:\n",
    "1. **Maximum Likelihood Estimation (MLE)**: Use only the observed data\n",
    "2. **Maximum A Posteriori (MAP)**: Combine data with prior beliefs\n",
    "3. **Posterior Mean**: Average over all hypotheses weighted by their posterior probability\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "The simplest approach is **Maximum Likelihood Estimation**: find the value of $\\theta$ that makes the observed data most probable.\n",
    "\n",
    "For coin flips, if we observe $k$ heads in $n$ flips, the likelihood of this data given $\\theta$ is:\n",
    "\n",
    "$$P(\\text{data} | \\theta) = \\theta^k (1-\\theta)^{n-k}$$\n",
    "\n",
    "The MLE estimate is the $\\theta$ that maximizes this likelihood. Using calculus, this turns out to be simply:\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\frac{k}{n} = \\frac{\\text{number of heads}}{\\text{total flips}}$$\n",
    "\n",
    "**Key property**: MLE uses only the observed data — no prior beliefs factor in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 1A\n",
    "\n",
    "Implement a function that computes the MLE estimate of $\\theta$ from a sequence of coin flips.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_estimate(sequence):\n",
    "    \"\"\"Compute the Maximum Likelihood Estimate of theta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : numpy array of 1s and 0s\n",
    "        The observed sequence of coin flips (1 = heads, 0 = tails)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The MLE estimate of theta (probability of heads)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "assert mle_estimate(np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1])) == 0.8\n",
    "assert mle_estimate(np.array([1, 1, 1, 1, 1])) == 1.0\n",
    "assert mle_estimate(np.array([0, 0, 0, 0, 0])) == 0.0\n",
    "assert mle_estimate(np.array([1, 0])) == 0.5\n",
    "assert mle_estimate(np.array([1, 0, 1, 0, 1, 0])) == 0.5\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 1B\n",
    "\n",
    "The code below plots how the MLE estimate evolves as we observe more flips. Two sequences are shown:\n",
    "- One sequence is all heads (MLE converges to 1.0)\n",
    "- The other is all tails (MLE converges to 0.0)\n",
    "\n",
    "**Your task**: Add axis labels, a title, and a legend to make this a complete, interpretable figure.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How MLE evolves as we observe more data\n",
    "n_flips = 10\n",
    "observations = np.arange(n_flips + 1)  # 0 to n_flips observations\n",
    "\n",
    "# Sequence 1: all heads\n",
    "sequence1 = np.zeros(n_flips)\n",
    "mle_seq1 = np.zeros(n_flips + 1)\n",
    "mle_seq1[0] = 0.5  # Before any data, undefined (we'll use 0.5)\n",
    "\n",
    "# Sequence 2: all tails\n",
    "sequence2 = np.ones(n_flips)\n",
    "mle_seq2 = np.zeros(n_flips + 1)\n",
    "mle_seq2[0] = 0.5\n",
    "\n",
    "for i in range(n_flips):\n",
    "    sequence1[i] = 1  # Add a head\n",
    "    sequence2[i] = 0  # Add a tail\n",
    "    mle_seq1[i+1] = mle_estimate(sequence1[:i+1])\n",
    "    mle_seq2[i+1] = mle_estimate(sequence2[:i+1])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(observations, mle_seq1, 'b-o', linewidth=2, markersize=6, label='All heads')\n",
    "plt.plot(observations, mle_seq2, 'r-o', linewidth=2, markersize=6, label='All tails')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "# YOUR CODE HERE: Add xlabel, ylabel, title, legend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. The Beta-Bernoulli Model\n",
    "\n",
    "MLE has a problem: with few observations, it can give extreme estimates. If you flip a coin 3 times and get 3 heads, MLE says $\\theta = 1$ — the coin always lands heads! This seems overconfident.\n",
    "\n",
    "**Bayesian inference** addresses this by incorporating **prior beliefs**. We use:\n",
    "\n",
    "- **Likelihood**: Bernoulli (each flip is independent with probability $\\theta$)\n",
    "- **Prior**: Beta distribution, $\\text{Beta}(\\alpha, \\beta)$\n",
    "\n",
    "The Beta distribution is a probability distribution over $\\theta \\in [0, 1]$. The parameters $\\alpha$ and $\\beta$ can be thought of as **pseudocounts**:\n",
    "- $\\alpha$ = number of \"prior heads\" we've imagined seeing + 1\n",
    "- $\\beta$ = number of \"prior tails\" we've imagined seeing + 1\n",
    "\n",
    "Special cases:\n",
    "- $\\text{Beta}(1, 1)$ = Uniform (no prior preference)\n",
    "- $\\text{Beta}(2, 2)$ = Slight preference for $\\theta = 0.5$\n",
    "- $\\text{Beta}(10, 10)$ = Strong belief that $\\theta \\approx 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different Beta priors\n",
    "theta_values = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "priors = [\n",
    "    (1, 1, 'Uniform: Beta(1,1)'),\n",
    "    (2, 2, 'Weak prior for 0.5: Beta(2,2)'),\n",
    "    (5, 5, 'Moderate prior for 0.5: Beta(5,5)'),\n",
    "    (10, 2, 'Prior favoring heads: Beta(10,2)'),\n",
    "    (2, 10, 'Prior favoring tails: Beta(2,10)')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for alpha, beta, label in priors:\n",
    "    pdf = stats.beta(alpha, beta).pdf(theta_values)\n",
    "    plt.plot(theta_values, pdf, linewidth=2, label=label)\n",
    "\n",
    "plt.xlabel(r'$\\theta$ (probability of heads)', fontsize=12)\n",
    "plt.ylabel('Prior probability density', fontsize=12)\n",
    "plt.title('Different Beta Prior Distributions', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Posterior Distribution\n",
    "\n",
    "After observing $k$ heads and $n-k$ tails, Bayes' rule gives us the **posterior distribution**:\n",
    "\n",
    "$$P(\\theta | \\text{data}) \\propto P(\\text{data} | \\theta) \\cdot P(\\theta)$$\n",
    "\n",
    "For the Beta-Bernoulli model, the posterior is also a Beta distribution:\n",
    "\n",
    "$$\\text{Posterior} = \\text{Beta}(\\alpha + k, \\beta + (n-k))$$\n",
    "\n",
    "The prior pseudocounts simply get added to the observed counts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the posterior updates with data\n",
    "# Prior: Beta(2, 2) - slight preference for fair coin\n",
    "alpha_prior, beta_prior = 2, 2\n",
    "\n",
    "# Observed data: 7 heads out of 10 flips\n",
    "n_heads, n_tails = 7, 3\n",
    "\n",
    "# Posterior parameters\n",
    "alpha_post = alpha_prior + n_heads\n",
    "beta_post = beta_prior + n_tails\n",
    "\n",
    "theta_values = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot prior\n",
    "prior_pdf = stats.beta(alpha_prior, beta_prior).pdf(theta_values)\n",
    "plt.plot(theta_values, prior_pdf, 'b-', linewidth=2, \n",
    "         label=f'Prior: Beta({alpha_prior}, {beta_prior})')\n",
    "\n",
    "# Plot likelihood (normalized for visualization)\n",
    "likelihood = theta_values**n_heads * (1-theta_values)**n_tails\n",
    "likelihood = likelihood / np.max(likelihood) * np.max(prior_pdf)  # Scale for visibility\n",
    "plt.plot(theta_values, likelihood, 'g--', linewidth=2, \n",
    "         label=f'Likelihood (scaled): {n_heads}H, {n_tails}T')\n",
    "\n",
    "# Plot posterior\n",
    "posterior_pdf = stats.beta(alpha_post, beta_post).pdf(theta_values)\n",
    "plt.plot(theta_values, posterior_pdf, 'r-', linewidth=2, \n",
    "         label=f'Posterior: Beta({alpha_post}, {beta_post})')\n",
    "\n",
    "# Mark MLE\n",
    "mle = n_heads / (n_heads + n_tails)\n",
    "plt.axvline(x=mle, color='green', linestyle=':', alpha=0.7, label=f'MLE = {mle:.2f}')\n",
    "\n",
    "plt.xlabel(r'$\\theta$ (probability of heads)', fontsize=12)\n",
    "plt.ylabel('Probability density', fontsize=12)\n",
    "plt.title('Bayesian Updating: Prior × Likelihood ∝ Posterior', fontsize=14)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "The **MAP estimate** is the value of $\\theta$ that maximizes the posterior distribution:\n",
    "\n",
    "$$\\hat{\\theta}_{MAP} = \\frac{\\alpha + k - 1}{\\alpha + \\beta + n - 2}$$\n",
    "\n",
    "where $k$ = observed heads, $n$ = total flips, and $\\alpha, \\beta$ are the prior parameters. \n",
    "\n",
    "**Note**: When $\\alpha = \\beta = 1$ (uniform prior), MAP reduces to MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 2A\n",
    "\n",
    "Implement a function that computes the MAP estimate of $\\theta$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_estimate(sequence, alpha_parameter=1, beta_parameter=1):\n",
    "    \"\"\"Compute the Maximum A Posteriori estimate of theta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : numpy array of 1s and 0s\n",
    "        The observed sequence of coin flips\n",
    "    alpha_parameter : float\n",
    "        Prior pseudocount for heads (prior_heads + 1)\n",
    "    beta_parameter : float\n",
    "        Prior pseudocount for tails (prior_tails + 1)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The MAP estimate of theta\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "sequence = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1])  # 8 heads, 2 tails\n",
    "\n",
    "# With uniform prior (alpha=1, beta=1), MAP should equal MLE\n",
    "assert map_estimate(sequence, 1, 1) == mle_estimate(sequence), \"With uniform prior, MAP should equal MLE\"\n",
    "\n",
    "# Test with different priors\n",
    "# MAP = (alpha + k - 1) / (alpha + beta + n - 2)\n",
    "assert abs(map_estimate(sequence, 2, 2) - 9/12) < 0.01  # (2+8-1)/(2+2+10-2) = 9/12 = 0.75\n",
    "assert abs(map_estimate(sequence, 5, 5) - 12/18) < 0.01  # (5+8-1)/(5+5+10-2) = 12/18\n",
    "assert abs(map_estimate(np.array([]), 5, 5) - 0.5) < 0.01  # No data: (5-1)/(5+5-2) = 4/8 = 0.5\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 2B\n",
    "\n",
    "The code below shows how MAP estimates evolve with data for different priors. \n",
    "\n",
    "**Your task**: Complete the figure by adding axis labels, title, and legend. Then answer: How does the strength of the prior affect how quickly the estimate converges to the MLE?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How different priors affect MAP estimates as data accumulates\n",
    "n_flips = 20\n",
    "\n",
    "# All heads sequence (extreme case)\n",
    "sequence = np.ones(n_flips)\n",
    "\n",
    "priors = [\n",
    "    (1, 1, 'Uniform prior'),\n",
    "    (2, 2, 'Weak prior (α=β=2)'),\n",
    "    (10, 10, 'Strong prior (α=β=10)')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for alpha, beta, label in priors:\n",
    "    estimates = []\n",
    "    for i in range(n_flips + 1):\n",
    "        if i == 0:\n",
    "            # Prior mode (before any data)\n",
    "            est = (alpha - 1) / (alpha + beta - 2) if alpha > 1 and beta > 1 else 0.5\n",
    "        else:\n",
    "            est = map_estimate(sequence[:i], alpha, beta)\n",
    "        estimates.append(est)\n",
    "    plt.plot(range(n_flips + 1), estimates, 'o-', linewidth=2, markersize=5, label=label)\n",
    "\n",
    "plt.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='True θ = 1.0')\n",
    "plt.xticks(range(0, n_flips + 1, 1))\n",
    "\n",
    "# YOUR CODE HERE: Add xlabel, ylabel, title, legend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER - Exercise 2B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Posterior Mean Estimation\n",
    "\n",
    "Instead of finding the mode (peak) of the posterior, we can compute its **mean**:\n",
    "\n",
    "$$\\hat{\\theta}_{PM} = \\frac{\\alpha + k}{\\alpha + \\beta + n}$$\n",
    "\n",
    "Keep in mind that $\\alpha$ = number of \"prior heads\" we've imagined seeing + 1 and $\\beta$ = number of \"prior tails\" we've imagined seeing + 1\n",
    "\n",
    "The posterior mean has nice properties:\n",
    "- It minimizes expected squared error\n",
    "- It's always well-defined (unlike MAP for uniform priors with no data)\n",
    "- It's a weighted average between the prior mean and the MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 3A\n",
    "\n",
    "Implement a function that computes the posterior mean estimate of $\\theta$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean(sequence, alpha_parameter=1, beta_parameter=1):\n",
    "    \"\"\"Compute the posterior mean estimate of theta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : numpy array of 1s and 0s\n",
    "        The observed sequence of coin flips\n",
    "    prior_heads : float\n",
    "        Prior pseudocount for heads (alpha parameter)\n",
    "    prior_tails : float\n",
    "        Prior pseudocount for tails (beta parameter)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The posterior mean estimate of theta\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "sequence = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1])  # 8 heads, 2 tails\n",
    "\n",
    "assert abs(posterior_mean(sequence, 1, 1) - 9/12) < 0.01  # (8+1)/(10+2)\n",
    "assert abs(posterior_mean(sequence, 2, 2) - 10/14) < 0.01  # (8+2)/(10+4)\n",
    "assert abs(posterior_mean(np.array([]), 5, 5) - 0.5) < 0.01  # Prior mean with no data\n",
    "assert abs(posterior_mean(np.array([1, 1, 1, 1, 1, 0]), 1, 1) - 6/8) < 0.01\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 3B\n",
    "\n",
    "Create a figure comparing MLE, MAP, and Posterior Mean estimates as data accumulates. Use a sequence that starts with 5 tails, then 15 heads. Use a moderate prior of Beta(5, 5).\n",
    "\n",
    "**Your task**: Write the code to generate this comparison plot with proper labels.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. The Coin of Azeroth: Putting It All Together\n",
    "\n",
    "Let's return to our original question. You flip the coin of Azeroth 10 times and observe:\n",
    "\n",
    "**H H T H T H H H H H** (8 heads, 2 tails)\n",
    "\n",
    "Based on prior experience with coins of Azeroth, you believe they tend to be slightly biased toward heads. You encode this as a prior of Beta(56, 46) — as if you've previously seen 100 flips with 55 heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observed sequence\n",
    "sequence = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Prior based on experience with Azeroth coins\n",
    "alpha_parameter = 56\n",
    "beta_parameter = 46\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"COIN OF AZEROTH ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nObserved: {int(np.sum(sequence))} heads, {int(len(sequence) - np.sum(sequence))} tails\")\n",
    "print(f\"Prior: Beta({alpha_parameter}, {beta_parameter}) — prior mean = {alpha_parameter/(alpha_parameter+beta_parameter):.3f}\")\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"{'Method':<20} {'Estimate':>15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'MLE':<20} {mle_estimate(sequence):>15.4f}\")\n",
    "print(f\"{'MAP':<20} {map_estimate(sequence, alpha_parameter, beta_parameter):>15.4f}\")\n",
    "print(f\"{'Posterior Mean':<20} {posterior_mean(sequence, alpha_parameter, beta_parameter):>15.4f}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nFather's requirement: θ between 0.55 and 0.75\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "Based on the results above:\n",
    "\n",
    "1. Should you take this coin for your father? Why or why not?\n",
    "\n",
    "2. The MLE and Bayesian estimates give very different answers. Why is this? Which do you trust more in this situation?\n",
    "\n",
    "3. If you could flip the coin 100 more times, would you expect the three estimates to become more similar or more different? Explain.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS - Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Visualizing Uncertainty: The Full Posterior (Optional)\n",
    "## Please note: this section is optional, but there is a Part 2 of the problem set below that is not optional!\n",
    "\n",
    "Point estimates (MLE, MAP, posterior mean) give us a single number. But the full posterior distribution tells us much more — it shows our **uncertainty** about $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full posterior for the coin of Azeroth\n",
    "sequence = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1])\n",
    "n_heads = int(np.sum(sequence))\n",
    "n_tails = len(sequence) - n_heads\n",
    "\n",
    "# Prior and posterior parameters\n",
    "alpha_prior, beta_prior = 56, 46\n",
    "alpha_post = alpha_prior + n_heads\n",
    "beta_post = beta_prior + n_tails\n",
    "\n",
    "theta = np.linspace(0, 1, 500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Prior vs Posterior\n",
    "ax = axes[0]\n",
    "ax.plot(theta, stats.beta(alpha_prior, beta_prior).pdf(theta), 'b-', linewidth=2, label='Prior')\n",
    "ax.plot(theta, stats.beta(alpha_post, beta_post).pdf(theta), 'r-', linewidth=2, label='Posterior')\n",
    "ax.axvline(x=posterior_mean(sequence, alpha_prior, beta_prior), color='r', linestyle='--', \n",
    "           alpha=0.7, label=f'Post. Mean = {posterior_mean(sequence, alpha_prior, beta_prior):.3f}')\n",
    "ax.axvspan(0.55, 0.75, alpha=0.2, color='green', label=\"Father's range [0.55, 0.75]\")\n",
    "ax.set_xlabel(r'$\\theta$', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Prior and Posterior Distributions', fontsize=13)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0.3, 0.9)\n",
    "\n",
    "# Right plot: Posterior with credible interval\n",
    "ax = axes[1]\n",
    "posterior = stats.beta(alpha_post, beta_post)\n",
    "ax.plot(theta, posterior.pdf(theta), 'r-', linewidth=2)\n",
    "\n",
    "# 95% credible interval\n",
    "ci_low, ci_high = posterior.ppf(0.025), posterior.ppf(0.975)\n",
    "theta_ci = theta[(theta >= ci_low) & (theta <= ci_high)]\n",
    "ax.fill_between(theta_ci, posterior.pdf(theta_ci), alpha=0.3, color='red',\n",
    "                label=f'95% CI: [{ci_low:.3f}, {ci_high:.3f}]')\n",
    "\n",
    "ax.axvspan(0.55, 0.75, alpha=0.2, color='green', label=\"Father's range\")\n",
    "ax.set_xlabel(r'$\\theta$', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Posterior with 95% Credible Interval', fontsize=13)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0.3, 0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate probability that theta is in father's range\n",
    "prob_in_range = posterior.cdf(0.75) - posterior.cdf(0.55)\n",
    "print(f\"\\nProbability that θ is in [0.55, 0.75]: {prob_in_range:.3f} ({prob_in_range*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "Explore how the posterior changes with different priors and data:\n",
    "\n",
    "1. Create a figure showing posteriors for the same data (8H, 2T) but with three different priors: weak (α=β=2), moderate (α=β=10), and strong (α=β=50, centered at 0.5). How does prior strength affect uncertainty?\n",
    "\n",
    "2. Create a figure showing how the posterior evolves as you observe more data. Start with the Beta(56,46) prior, and show the posterior after 10, 50, and 200 flips (assume 70% heads throughout).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Exercise 5.1: Different prior strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Exercise 5.2: Posterior evolution with more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR NOTES - Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Bayesian Occam's Razor\n",
    "\n",
    "In Part 1, we estimated a continuous parameter ($\\theta$) from data. Now we'll explore a different problem: **hypothesis selection**. Given some observations, which hypothesis best explains the data?\n",
    "\n",
    "Bayesian inference automatically implements **Occam's Razor** — the principle that simpler explanations should be preferred, all else being equal. But \"simpler\" has a precise meaning: hypotheses that make **sharper predictions** (assign probability to fewer outcomes) are rewarded when those predictions are correct.\n",
    "\n",
    "## 7.1 The Image Classification Problem\n",
    "\n",
    "Imagine you're trying to learn a concept from examples, like the example in class. A teacher shows you images, and you must figure out what concept they're teaching.\n",
    "\n",
    "We have **12 images** with nested features:\n",
    "- Images 1-12: All are outdoor scenes\n",
    "- Images 1-9: Include a mammal\n",
    "- Images 1-6: Include a large mammal  \n",
    "- Images 1-3: Include a horse\n",
    "- Images 1-2: Include a **running** horse\n",
    "\n",
    "Each feature corresponds to a hypothesis about the concept the teacher might be teaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the nested structure of hypotheses\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the hypotheses and which images they cover\n",
    "hypotheses = {\n",
    "    'Outdoors': list(range(1, 13)),      # Images 1-12\n",
    "    'Mammal': list(range(1, 10)),         # Images 1-9\n",
    "    'Large mammal': list(range(1, 7)),    # Images 1-6\n",
    "    'Horse': list(range(1, 4)),           # Images 1-3\n",
    "    'Running horse': list(range(1, 3))    # Images 1-2\n",
    "}\n",
    "\n",
    "colors = ['#E8E8E8', '#FFD700', '#FFA500', '#FF6347', '#DC143C']\n",
    "y_positions = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Draw rectangles for each hypothesis\n",
    "for (name, images), color, y in zip(hypotheses.items(), colors, y_positions):\n",
    "    width = len(images)\n",
    "    rect = mpatches.FancyBboxPatch((0.5, y - 0.35), width, 0.7, \n",
    "                                    boxstyle=\"round,pad=0.02\", \n",
    "                                    facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(width/2 + 0.5, y, f'{name}\\n({len(images)} images)', \n",
    "            ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Draw image markers\n",
    "for i in range(1, 13):\n",
    "    ax.plot(i, -1, 's', markersize=20, color='lightblue', markeredgecolor='black')\n",
    "    ax.text(i, -1, str(i), ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.text(6.5, -1.8, 'Images', ha='center', fontsize=12)\n",
    "\n",
    "ax.set_xlim(0, 13)\n",
    "ax.set_ylim(-2.5, 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Nested Hypothesis Structure', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 The Key Insight: Specificity vs. Risk\n",
    "\n",
    "Each hypothesis defines a **likelihood function** — the probability of observing each image given that hypothesis:\n",
    "\n",
    "$$P(\\text{image } i \\mid H) = \\begin{cases} \\frac{1}{|H|} & \\text{if image } i \\in H \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "where $|H|$ is the number of images consistent with hypothesis $H$.\n",
    "\n",
    "**The tradeoff:**\n",
    "- **Specific hypotheses** (like \"running horse\") assign high probability to each consistent image (1/2 each)\n",
    "- **General hypotheses** (like \"outdoors\") spread probability thinly (1/12 each)\n",
    "\n",
    "But specific hypotheses are **riskier** — they're wrong for more images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hypotheses\n",
    "hypotheses = {\n",
    "    'Outdoors': set(range(1, 13)),       # Images 1-12\n",
    "    'Mammal': set(range(1, 10)),          # Images 1-9\n",
    "    'Large mammal': set(range(1, 7)),     # Images 1-6\n",
    "    'Horse': set(range(1, 4)),            # Images 1-3\n",
    "    'Running horse': set(range(1, 3))     # Images 1-2\n",
    "}\n",
    "\n",
    "def likelihood(image, hypothesis_name):\n",
    "    \"\"\"P(image | hypothesis) - probability of seeing this image given the hypothesis.\"\"\"\n",
    "    h_images = hypotheses[hypothesis_name]\n",
    "    if image in h_images:\n",
    "        return 1.0 / len(h_images)  # Uniform over consistent images\n",
    "    else:\n",
    "        return 0.0  # Impossible under this hypothesis\n",
    "\n",
    "# Show likelihoods for a few example images\n",
    "print(\"Likelihood P(image | hypothesis) for different images:\\n\")\n",
    "print(f\"{'Hypothesis':<15} {'Image 1':>10} {'Image 2':>10} {'Image 3':>10} {'Image 7':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for h in hypotheses.keys():\n",
    "    l1 = likelihood(1, h)\n",
    "    l2 = likelihood(2, h)\n",
    "    l3 = likelihood(3, h)\n",
    "    l7 = likelihood(7, h)\n",
    "    print(f\"{h:<15} {l1:>10.3f} {l2:>10.3f} {l3:>10.3f} {l7:>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Bayesian Hypothesis Testing\n",
    "\n",
    "Given observed images, we update our beliefs about which hypothesis is correct using Bayes' rule:\n",
    "\n",
    "$$P(H \\mid \\text{data}) \\propto P(\\text{data} \\mid H) \\cdot P(H)$$\n",
    "\n",
    "We'll start with a **uniform prior** (all hypotheses equally likely) and see how observations shift our beliefs.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Exercise 6\n",
    "\n",
    "Implement the `compute_posterior` function below to calculate the posterior probability over hypotheses given observed images.\n",
    "\n",
    "**Steps:**\n",
    "1. Compute the likelihood of all observations for each hypothesis (multiply the individual likelihoods)\n",
    "2. Compute the unnormalized posterior = likelihood × prior\n",
    "3. Normalize so the posteriors sum to 1\n",
    "\n",
    "**Hint:** Use the `likelihood(image, hypothesis_name)` function defined above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(observed_images, prior=None):\n",
    "    \"\"\"Compute posterior probability over hypotheses given observed images.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observed_images : list of int\n",
    "        The images that were observed\n",
    "    prior : dict, optional\n",
    "        Prior probability for each hypothesis. Default is uniform.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    posterior : dict\n",
    "        Posterior probability for each hypothesis\n",
    "    likelihoods : dict\n",
    "        Likelihood of the data for each hypothesis\n",
    "    \"\"\"\n",
    "    if prior is None:\n",
    "        prior = {h: 1/len(hypotheses) for h in hypotheses}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "def plot_posterior(observed_images, ax=None):\n",
    "    \"\"\"Plot the posterior distribution over hypotheses.\"\"\"\n",
    "    posterior, likelihoods = compute_posterior(observed_images)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    h_names = list(hypotheses.keys())\n",
    "    probs = [posterior[h] for h in h_names]\n",
    "    colors = ['#E8E8E8', '#FFD700', '#FFA500', '#FF6347', '#DC143C']\n",
    "    \n",
    "    bars = ax.bar(h_names, probs, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax.set_ylabel('Posterior Probability', fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f'Observed: Image(s) {observed_images}', fontsize=13)\n",
    "    \n",
    "    # Add probability labels on bars\n",
    "    for bar, prob in zip(bars, probs):\n",
    "        if prob > 0.01:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                   f'{prob:.2f}', ha='center', fontsize=10)\n",
    "    \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "posterior, likelihoods = compute_posterior([1])\n",
    "\n",
    "# Check that posterior sums to 1\n",
    "assert abs(sum(posterior.values()) - 1.0) < 0.001, \"Posterior should sum to 1\"\n",
    "\n",
    "# Check specific values for observing image 1\n",
    "# Running horse has likelihood 1/2, should have highest posterior\n",
    "assert posterior['Running horse'] > posterior['Horse'], \"Running horse should beat Horse for image 1\"\n",
    "assert posterior['Horse'] > posterior['Large mammal'], \"Horse should beat Large mammal for image 1\"\n",
    "\n",
    "# Check that likelihoods are computed correctly\n",
    "assert abs(likelihoods['Running horse'] - 0.5) < 0.001, \"P(image 1 | Running horse) = 1/2\"\n",
    "assert abs(likelihoods['Horse'] - 1/3) < 0.001, \"P(image 1 | Horse) = 1/3\"\n",
    "assert abs(likelihoods['Outdoors'] - 1/12) < 0.001, \"P(image 1 | Outdoors) = 1/12\"\n",
    "\n",
    "# Test with image 3 (which rules out Running horse)\n",
    "posterior3, likelihoods3 = compute_posterior([3])\n",
    "assert likelihoods3['Running horse'] == 0, \"Image 3 is not consistent with Running horse\"\n",
    "assert posterior3['Running horse'] == 0, \"Running horse posterior should be 0 for image 3\"\n",
    "assert posterior3['Horse'] > posterior3['Large mammal'], \"Horse should have highest posterior for image 3\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Example: Single Observation\n",
    "\n",
    "Let's see what happens when we observe a single image. Watch how the **most specific consistent hypothesis** gets the highest posterior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare posteriors for different single observations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "observations = [[1], [2], [3], [7]]\n",
    "titles = ['Image 1 (running horse)', 'Image 2 (running horse)', \n",
    "          'Image 3 (horse, not running)', 'Image 7 (mammal, not large)']\n",
    "\n",
    "for ax, obs, title in zip(axes.flat, observations, titles):\n",
    "    plot_posterior(obs, ax)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: The most SPECIFIC hypothesis consistent with the data wins!\")\n",
    "print(\"This is Bayesian Occam's Razor in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Multiple Observations: Evidence Accumulates\n",
    "\n",
    "With multiple observations, the posterior reflects which hypothesis best explains **all** the data. Hypotheses that are inconsistent with even one observation get eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how posterior evolves with multiple observations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Scenario 1: Observing images 1, then 2 (both running horses)\n",
    "observation_sequences = [\n",
    "    ([1], 'After seeing image 1'),\n",
    "    ([1, 2], 'After seeing images 1, 2'),\n",
    "    ([1, 2, 1], 'After seeing images 1, 2, 1'),\n",
    "]\n",
    "\n",
    "# Scenario 2: Observing images 1, then 3 (running horse rules out)\n",
    "observation_sequences2 = [\n",
    "    ([1], 'After seeing image 1'),\n",
    "    ([1, 3], 'After seeing images 1, 3'),\n",
    "    ([1, 3, 2], 'After seeing images 1, 3, 2'),\n",
    "]\n",
    "\n",
    "for i, (obs, title) in enumerate(observation_sequences):\n",
    "    plot_posterior(obs, axes[0, i])\n",
    "    axes[0, i].set_title(title, fontsize=10)\n",
    "    axes[0, i].tick_params(axis='x', rotation=30)\n",
    "\n",
    "for i, (obs, title) in enumerate(observation_sequences2):\n",
    "    plot_posterior(obs, axes[1, i])\n",
    "    axes[1, i].set_title(title, fontsize=10)\n",
    "    axes[1, i].tick_params(axis='x', rotation=30)\n",
    "\n",
    "axes[0, 0].set_ylabel('Scenario 1\\n(all running horses)\\n\\nPosterior Prob.', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Scenario 2\\n(image 3 rules out\\nrunning horse)\\n\\nPosterior Prob.', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Example: The Three Horse Images\n",
    "\n",
    "Suppose the teacher shows you images 1, 2, and 3 — exactly the three horse images. What should you conclude?\n",
    "\n",
    "Let's compute the posterior step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The teacher shows images 1, 2, and 3 (all three horse images)\n",
    "observed = [1, 2, 3]\n",
    "\n",
    "# Compute posterior\n",
    "posterior, likelihoods = compute_posterior(observed)\n",
    "\n",
    "# Show the likelihood calculation\n",
    "print(\"Step 1: Compute likelihood P(data | H) for each hypothesis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Data observed: images {observed}\\n\")\n",
    "\n",
    "for h, h_images in hypotheses.items():\n",
    "    n = len(h_images)\n",
    "    # Check if all observed images are in this hypothesis\n",
    "    all_consistent = all(img in h_images for img in observed)\n",
    "    \n",
    "    if all_consistent:\n",
    "        lik = (1/n) ** len(observed)\n",
    "        print(f\"{h:<15}: Each image has P = 1/{n}\")\n",
    "        print(f\"               Likelihood = (1/{n})^3 = {lik:.6f}\")\n",
    "    else:\n",
    "        print(f\"{h:<15}: Image 3 is NOT in this set → Likelihood = 0\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nStep 2: Apply Bayes' rule (uniform prior)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"P(H | data) ∝ P(data | H) × P(H)\")\n",
    "print(\"With uniform prior, posterior ∝ likelihood\\n\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_posterior(observed, ax)\n",
    "ax.set_title('Posterior after observing images 1, 2, 3 (the three horses)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPosterior probabilities:\")\n",
    "print(\"-\" * 40)\n",
    "for h in hypotheses:\n",
    "    print(f\"{h:<15}: {posterior[h]:.4f}\")\n",
    "    \n",
    "print(f\"\\n→ The 'Horse' hypothesis wins with {posterior['Horse']:.1%} probability!\")\n",
    "print(\"  It's the most SPECIFIC hypothesis consistent with ALL the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## 7.7 Exercise 7\n",
    "\n",
    "Explore the Bayesian Occam's razor effect:\n",
    "\n",
    "1. What happens if you observe images [4, 5, 6]? Which hypothesis wins? Why?\n",
    "\n",
    "2. What's the minimum number of observations needed to be 90% confident in the \"Horse\" hypothesis?\n",
    "\n",
    "3. Suppose the teacher is actually teaching \"Large mammal\". Simulate observing random images from this concept (images 1-6) and plot how the posterior evolves. How many observations does it typically take to correctly identify the hypothesis?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS - Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Why Does Occam's Razor Work?\n",
    "\n",
    "The Bayesian explanation is elegant:\n",
    "\n",
    "1. **Specific hypotheses make sharper predictions** — they assign higher probability to each consistent outcome\n",
    "2. **When the prediction is correct**, this higher likelihood translates to higher posterior probability\n",
    "3. **But specific hypotheses are also riskier** — they rule out more possibilities\n",
    "\n",
    "The math automatically balances these tradeoffs. You don't need to manually penalize complex hypotheses — the likelihood function does it for you!\n",
    "\n",
    "$$\\underbrace{P(H|\\text{data})}_{\\text{posterior}} \\propto \\underbrace{P(\\text{data}|H)}_{\\text{rewards specificity}} \\times \\underbrace{P(H)}_{\\text{prior}}$$\n",
    "\n",
    "This is sometimes called the **size principle**: among hypotheses consistent with the data, prefer the smallest (most specific) one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
