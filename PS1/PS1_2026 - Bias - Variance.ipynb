{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8619ea68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "# PS 1 - Bias and variance\n",
    "\n",
    "In this problem set, we will replicate the code from lecture 2, to visualize bias and variance in model fitting, and get an understanding of what overfitting means.\n",
    "\n",
    "## Reminder -  problem\n",
    "\n",
    "We are trying to decide how well a model fits data. We assume our data is a set of $(x_i,y_i)$ points (e.g. age, task accuracy) generated by a true target function $f$ (unknown; for example, accuracy in a task as a function of age): $$y_1 \\sim f(x_i) + noise$$\n",
    "Our goal is to figure out $f$ (for example, does task accuracy increase linearly with age, or is the link more complex?).\n",
    "\n",
    "\n",
    "## Reminder - approach\n",
    "We model it with a proposed \"computational model\", which is here a function $g(x)$, for example a linear regression (2 parameters: intercept, slope), or an n degree polynomial function ($n+1$ parameters). \n",
    "\n",
    "After we find the best parameters for a model, we can measure how well our model is doing by \n",
    "- checking its prediction for each data point in the training set (e.g. how well do I expect a 16 year-old to do in the task?): $g(x_i)$. \n",
    "- computing the prediction error for this data point as the distance to the true value $(y_i-g(x_i))^2$.\n",
    "- averaging this error over all the data points: $Error_{train} = \\frac{1}{N}\\sum_i ^N (y_i-g(x_i))^2$.\n",
    "\n",
    "This tells us how close we are to the training data, but not the true function. To see that, we want to measure how well we can predict *new* test data, that is a new set of points $(x^{test} _i, y^{test} _i)$. We can compute the testing error the same way: $Error_{test} = \\frac{1}{N}\\sum_i ^N (y^{test} _i-g(x^{test} _i))^2$.\n",
    "\n",
    "## Reminder - Bias and variance\n",
    "If our training error is low but our testing error is high, it means our model gets close to the training data, but actually can't predict any new data. That's called \"overfitting\" : our model is too complex, captures the noise in the data instead of the signal. Instead, we want both the training and testing error to be low, which shows that the model can generalize to new data.\n",
    "\n",
    "We did some maths to show that the expected error can be divided into two terms: the bias and the variance:\n",
    "- the bias is the distance from the model to the true function, in expectation over noisy data sets [am I systematically not capturing the true phenomenon?]\n",
    "- the variance is the variance in the model itself [with different training data sets, do I make very different predictions?]\n",
    "\n",
    "\n",
    "## This problem set\n",
    "\n",
    "We'll use the target function from class, plot the results, and see how different assumptions (number of data points, amount of noise, etc.) impact the model and its fit. Your goal is to experience the phenomenon yourself to understand better what happens, and get a good feel for the concepts. Your goal is also to practice coding a model and good visualization. \n",
    "\n",
    "We haven't learned how to fit a model yet, so we're providing this here. You do not need to understand the model fitting part at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b12ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:21.273518Z",
     "iopub.status.busy": "2026-01-11T23:57:21.273250Z",
     "iopub.status.idle": "2026-01-11T23:57:22.072720Z",
     "shell.execute_reply": "2026-01-11T23:57:22.072417Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy as always\n",
    "import numpy as np\n",
    "# plotting!\n",
    "import matplotlib.pyplot as plt\n",
    "# the packages below are used to fit the polynomial models and generate predictions/measure errors from them. \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Constants for x-axis ranges ---\n",
    "X_TRAIN_MIN = 12  # Minimum age for training data\n",
    "X_TRAIN_MAX = 30  # Maximum age for training data\n",
    "X_PLOT_MIN = 10   # Minimum age for plotting (slightly wider to show edge behavior)\n",
    "X_PLOT_MAX = 32   # Maximum age for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121c85f",
   "metadata": {},
   "source": [
    "# Setting up the polynomial regression model for bias-variance\n",
    "\n",
    "In the code below, we will create a true function that we want to model (target_function, the 2nd degree polynomial from class that we were trying to model: accuracy as a function of age). We will create a noisy data set generated by this function, with different number of samples (n_samples) and noise levels (noise_level).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9617a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:22.074886Z",
     "iopub.status.busy": "2026-01-11T23:57:22.074681Z",
     "iopub.status.idle": "2026-01-11T23:57:22.078343Z",
     "shell.execute_reply": "2026-01-11T23:57:22.078029Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Setup Data Generation\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "def target_function(x):\n",
    "    # Relevant quadratic function: y =a+b x^2\n",
    "    return 1-.0025*(x-25)**2\n",
    "\n",
    "# Generate Training Data (10 points)\n",
    "n_samples = 10\n",
    "noise_level = .05  # Standard deviation of noise\n",
    "\n",
    "X_train = np.sort(np.random.uniform(X_TRAIN_MIN, X_TRAIN_MAX, n_samples))\n",
    "y_train = target_function(X_train) + np.random.normal(0, noise_level, n_samples)\n",
    "\n",
    "# Generate Test Data (10 points)\n",
    "X_test = np.sort(np.random.uniform(X_TRAIN_MIN, X_TRAIN_MAX, n_samples))# pick ages uniform from 12-30\n",
    "y_test = target_function(X_test) + np.random.normal(0, noise_level, n_samples)# generate data as target + noise\n",
    "\n",
    "# For plotting the smooth curves\n",
    "X_plot = np.linspace(X_PLOT_MIN, X_PLOT_MAX, 100)\n",
    "y_true_plot = target_function(X_plot)\n",
    "\n",
    "# Reshape X for sklearn (needs 2D array)\n",
    "X_train_r = X_train[:, np.newaxis]\n",
    "X_test_r = X_test[:, np.newaxis]\n",
    "X_plot_r = X_plot[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d182388",
   "metadata": {},
   "source": [
    "\n",
    "Same as in class, we will fit 3 models: a linear regression (2 parameters), a 2nd degree polynomial (3 parameters), and an 8th degree polynomial (9 parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6aa50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:22.079908Z",
     "iopub.status.busy": "2026-01-11T23:57:22.079792Z",
     "iopub.status.idle": "2026-01-11T23:57:22.081630Z",
     "shell.execute_reply": "2026-01-11T23:57:22.081391Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Define Models\n",
    "degrees = [1, 2, 8]\n",
    "model_names = [\"Linear (Underfitting)\", \"Quadratic (Optimal)\", \"8th Degree (Overfitting)\"]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ac01f",
   "metadata": {},
   "source": [
    "Below is the fitting code. Make sure you read and understand what each step is doing at the high level. At this stage of the class, you do not need to understand the details of how the sklearn package functions perform the fitting - you should just understand at the conceptual level what their output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a740a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:22.083360Z",
     "iopub.status.busy": "2026-01-11T23:57:22.083265Z",
     "iopub.status.idle": "2026-01-11T23:57:22.431101Z",
     "shell.execute_reply": "2026-01-11T23:57:22.430785Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use 'constrained_layout=True' to fix the tight spacing/overlap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), constrained_layout=True)\n",
    "# 3. Fit Models and Visualize\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax = axes[i] # Access the specific subplot\n",
    "    \n",
    "    # Create and fit the model\n",
    "    # obtain the best parameters for each model, given the data\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train_r, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    # obtain the predictions based on known x values (ages), and fit parameters (values on the fit curve)\n",
    "    y_plot_pred = model.predict(X_plot_r)\n",
    "    y_train_pred = model.predict(X_train_r)\n",
    "    y_test_pred = model.predict(X_test_r)\n",
    "    \n",
    "    # Calculate Errors\n",
    "    # distance between true data (accuracy) and predicted\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Plotting\n",
    "    ax.plot(X_plot, y_true_plot, color='gray', linestyle='--', label=\"True Function\")\n",
    "    ax.scatter(X_train, y_train, color='navy', s=50, label=\"Train Data\")\n",
    "    ax.scatter(X_test, y_test, color='orange', marker='x', s=50, label=\"Test Data\")\n",
    "    ax.plot(X_plot, y_plot_pred, color=colors[i], linewidth=2, label=f\"Model (deg={degree})\")\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(f\"{model_names[i]}\\nTrain MSE: {train_mse:.3f} | Test MSE: {test_mse:.3f}\")\n",
    "    ax.set_ylim(0.5, 1.1)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.xlabel('age')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0162c78",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# 1. Exercise\n",
    "\n",
    "1. fix the code above so that all three plots have an x and y label. \n",
    "2. change the random seed (or do not set it) to see what happens with different data sets. Which fit model changes more?\n",
    "3. change the noise level up and down. What happens to the training and testing error?\n",
    "4. change the number of training data points up and down. What happens to the training and testing error?\n",
    "\n",
    "Tips for playing with models:\n",
    "Consider:\n",
    "- creating new cells for each attempt, so you can compare the plots; \n",
    "- adding markdown cells with your notes/questions/conclusions to help you review your work;\n",
    "- adding titles that indicate the value of the variable you're investigating, so you know which plot is which; \n",
    "- and refactoring the code as functions so that you can call the function with parameter values of interest, rather than copy-pasting the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9cc78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:22.433405Z",
     "iopub.status.busy": "2026-01-11T23:57:22.433286Z",
     "iopub.status.idle": "2026-01-11T23:57:22.435114Z",
     "shell.execute_reply": "2026-01-11T23:57:22.434819Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d28b02",
   "metadata": {},
   "source": [
    "### YOUR NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03fd20",
   "metadata": {},
   "source": [
    "# 2. Iterating over multiple datasets\n",
    "\n",
    "Let's verify the insights from this first exercise one by visualizing them systematically: what happens if we train the model on different data sets?\n",
    "\n",
    "This will also allow us to formally compute the bias and variance, in expectation over training on multiple data sets. \n",
    "\n",
    "Make sure to carefully read through the code to understand what we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08501b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:22.436842Z",
     "iopub.status.busy": "2026-01-11T23:57:22.436734Z",
     "iopub.status.idle": "2026-01-11T23:57:22.439153Z",
     "shell.execute_reply": "2026-01-11T23:57:22.438885Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "np.random.seed(42)\n",
    "n_simulations = 100\n",
    "n_samples = 15        # Size of each training set\n",
    "noise_level = .05     # Irreducible error (sigma)\n",
    "degrees_to_plot = [1, 2, 8] # The specific models we want to visualize depth for\n",
    "max_degree_analysis = 9     # For the final tradeoff curve\n",
    "\n",
    "# Fixed Test Set (ground truth for evaluating bias/variance)\n",
    "X_test = np.linspace(X_PLOT_MIN, X_PLOT_MAX, 100)\n",
    "y_true = target_function(X_test)\n",
    "X_test_r = X_test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e42a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:22.440680Z",
     "iopub.status.busy": "2026-01-11T23:57:22.440592Z",
     "iopub.status.idle": "2026-01-11T23:57:22.894198Z",
     "shell.execute_reply": "2026-01-11T23:57:22.893729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Storage for predictions across all simulations\n",
    "# Structure: {degree: [pred_sim_1, pred_sim_2, ...]}\n",
    "all_predictions = {d: [] for d in range(1, max_degree_analysis + 1)}\n",
    "\n",
    "# --- 1. Run Simulations ---\n",
    "for i in range(n_simulations):\n",
    "    # Generate a fresh training set with random noise\n",
    "    X_train = np.sort(np.random.uniform(X_TRAIN_MIN, X_TRAIN_MAX, n_samples))\n",
    "    y_train = target_function(X_train) + np.random.normal(0, noise_level, n_samples)\n",
    "    X_train_r = X_train[:, np.newaxis]\n",
    "    \n",
    "    # Fit models of varying complexity\n",
    "    for d in range(1, max_degree_analysis + 1):\n",
    "        model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "        model.fit(X_train_r, y_train)\n",
    "        y_pred = model.predict(X_test_r)\n",
    "        all_predictions[d].append(y_pred)\n",
    "\n",
    "# Convert lists to numpy arrays for calculation\n",
    "for d in all_predictions:\n",
    "    all_predictions[d] = np.array(all_predictions[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396d8a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:22.896020Z",
     "iopub.status.busy": "2026-01-11T23:57:22.895903Z",
     "iopub.status.idle": "2026-01-11T23:57:23.263323Z",
     "shell.execute_reply": "2026-01-11T23:57:23.262966Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. Visualization Part A: Model Stability (Spaghetti Plots) ---\n",
    "plt.figure(figsize=(18, 5))\n",
    "titles = [\"Degree 1 (High Bias, Low Var)\", \"Degree 2 (Optimal)\", \"Degree 8 (Low Bias, High Var)\"]\n",
    "\n",
    "for i, d in enumerate(degrees_to_plot):\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    \n",
    "    # Plot the 100 different models generated\n",
    "    # We use high transparency (alpha) to show density\n",
    "    preds = all_predictions[d]\n",
    "    for j in range(n_simulations):\n",
    "        ax.plot(X_test, preds[j], color='blue', alpha=0.1, linewidth=1)\n",
    "        \n",
    "    # Plot the Average Model (Expected Value)\n",
    "    avg_pred = np.mean(preds, axis=0)\n",
    "    ax.plot(X_test, avg_pred, color='red', linewidth=3, linestyle='--', label=\"Average Model\")\n",
    "    \n",
    "    # Plot Truth\n",
    "    ax.plot(X_test, y_true, color='black', linewidth=2, label=\"True Function\")\n",
    "    \n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_ylim(.5, 1.1)\n",
    "    if i == 0: ax.legend()\n",
    "\n",
    "plt.suptitle(\"Visualizing Variance: 100 Simulations of Training Data\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fad3c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# 2.  Exercise\n",
    "\n",
    "Oh no! Poor plotting practices! What are the blue lines???? add to the legend, explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24181c",
   "metadata": {},
   "source": [
    "### Your Notes here\n",
    "\n",
    "Write your answers, thoughts, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44780278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:23.265826Z",
     "iopub.status.busy": "2026-01-11T23:57:23.265723Z",
     "iopub.status.idle": "2026-01-11T23:57:23.414108Z",
     "shell.execute_reply": "2026-01-11T23:57:23.413739Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Calculation & Visualization Part B: The Tradeoff Curve ---\n",
    "max_degree_for_plotting = 4\n",
    "degrees = range(1, max_degree_for_plotting + 1)\n",
    "bias_squared = []\n",
    "variance = []\n",
    "total_error = []\n",
    "\n",
    "for d in degrees:\n",
    "    preds = all_predictions[d] # Shape: (100 simulations, 100 test points)\n",
    "    \n",
    "    # Expected Prediction (Mean across simulations)\n",
    "    E_y_hat = np.mean(preds, axis=0)\n",
    "    \n",
    "    # Bias^2: (Expected_Pred - Truth)^2\n",
    "    # We take the mean over all test points to get a single scalar score\n",
    "    bias_sq = np.mean((E_y_hat - y_true) ** 2)\n",
    "    \n",
    "    # Variance: E[(Pred - Expected_Pred)^2]\n",
    "    # We take the mean variance over all test points\n",
    "    var = np.mean(np.var(preds, axis=0))\n",
    "    \n",
    "    bias_squared.append(bias_sq)\n",
    "    variance.append(var)\n",
    "    total_error.append(bias_sq + var + noise_level**2)\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, bias_squared, 'o-', label='$Bias^2$', color='blue', linewidth=2)\n",
    "plt.plot(degrees, variance, 'o-', label='Variance', color='orange', linewidth=2)\n",
    "plt.plot(degrees, total_error, 'o-', label='Total Error', color='red', linewidth=2)\n",
    "\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('The Bias-Variance Tradeoff')\n",
    "plt.axvline(x=2, color='gray', linestyle=':', label='Optimal Complexity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453e61e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# 3. Exercise\n",
    "\n",
    "Same as in exercise 1, manipulate noise and number of samples in the training data. You can also test more values of model complexity by changing max_degree_for_plotting.\n",
    "\n",
    "You should draw the same conclusions as in exercise 1, but see if the new visualizations make it more obvious/easy to interpret.\n",
    "\n",
    "\n",
    "Make sure this helps you understand the concepts of bias and variance to assess model fit, as well as overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e6fb2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# 4. Exercise\n",
    "\n",
    "The cell below defines a new, more complex target function. Add code to visualize this function. Reproduce the exercises above with the more complex function. What changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1de2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:57:23.416058Z",
     "iopub.status.busy": "2026-01-11T23:57:23.415917Z",
     "iopub.status.idle": "2026-01-11T23:57:23.418163Z",
     "shell.execute_reply": "2026-01-11T23:57:23.417807Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_target_function(x):\n",
    "    # Relevant degree 4 function\n",
    "    return 1-.0025*(x-25)**2 - .00005*(x-11)*(x-17)*(x-27)*(x-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80377373",
   "metadata": {},
   "source": [
    "### YOUR NOTES/ANSWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043567e",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
